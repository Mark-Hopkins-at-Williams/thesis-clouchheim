True
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training models/encrypted/TEST_2-v5
Augmenting vocabulary with the following tokens:
  por2_Latn
  por1_Latn
Validating on a sample...
dev loss: 11.62751132965088
Saving new best model!
step 500 (train): 8.771816013336181
Validating on a sample...
dev loss: 7.094360952377319
Saving new best model!
step 1000 (train): 6.553899688720703
Validating on a sample...
dev loss: 6.16719211101532
Saving new best model!
step 1500 (train): 5.791267637252807
Validating on a sample...
dev loss: 5.569935975074768
Saving new best model!
step 2000 (train): 5.347993497848511
Validating on a sample...
dev loss: 5.115770936012268
Saving new best model!
step 2500 (train): 4.9627423124313355
Validating on a sample...
dev loss: 4.78637704372406
Saving new best model!
step 3000 (train): 4.704794265270233
Validating on a sample...
dev loss: 4.532409744262695
Saving new best model!
step 3500 (train): 4.447677623748779
Validating on a sample...
dev loss: 4.2741959238052365
Saving new best model!
step 4000 (train): 4.230692076683044
Validating on a sample...
dev loss: 4.104096765518189
Saving new best model!
step 4500 (train): 4.138173491954803
Validating on a sample...
dev loss: 3.9475356698036195
Saving new best model!
step 5000 (train): 3.946253228664398
Validating on a sample...
dev loss: 3.844519679546356
Saving new best model!
step 5500 (train): 3.803397057056427
Validating on a sample...
dev loss: 3.646743221282959
