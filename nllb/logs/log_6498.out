huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Training models/encrypted/TEST-v21
Augmenting vocabulary with the following tokens:
  por1_Latn
  por2_Latn
Validating on a sample...
dev loss: 11.93799609184265
Saving new best model!
step 500 (train): 8.831942034721374
Validating on a sample...
dev loss: 7.073428106307984
Saving new best model!
step 1000 (train): 6.505709306716919
Validating on a sample...
dev loss: 6.051120476722717
Saving new best model!
step 1500 (train): 5.600686416625977
Validating on a sample...
dev loss: 5.165499653816223
Saving new best model!
step 2000 (train): 4.941658962726593
Validating on a sample...
dev loss: 4.582852699756622
Saving new best model!
step 2500 (train): 4.385493388175965
Validating on a sample...
dev loss: 4.077161588668823
Saving new best model!
step 3000 (train): 4.02934525346756
Validating on a sample...
dev loss: 3.614581162929535
Saving new best model!
step 3500 (train): 3.666446744918823
Validating on a sample...
dev loss: 3.3703521299362182
Saving new best model!
step 4000 (train): 3.382024297714233
Validating on a sample...
dev loss: 3.1497818851470947
Saving new best model!
step 4500 (train): 3.2412501616477964
Validating on a sample...
dev loss: 2.978380727767944
Saving new best model!
step 5000 (train): 3.0265575194358827
Validating on a sample...
dev loss: 2.7807756638526917
Saving new best model!
step 5500 (train): 2.849442071676254
Validating on a sample...
dev loss: 2.659630696773529
Saving new best model!
step 6000 (train): 2.72186256313324
Validating on a sample...
dev loss: 2.5428552174568178
Saving new best model!
step 6500 (train): 2.5518226997852325
Validating on a sample...
dev loss: 2.4588996601104736
Saving new best model!
step 7000 (train): 2.463956379175186
Validating on a sample...
dev loss: 2.3777484810352325
Saving new best model!
step 7500 (train): 2.4044525537490844
Validating on a sample...
dev loss: 2.2958229899406435
Saving new best model!
step 8000 (train): 2.3141443350315094
Validating on a sample...
dev loss: 2.2422348535060883
Saving new best model!
step 8500 (train): 2.2143696846961975
Validating on a sample...
dev loss: 2.1831066250801086
Saving new best model!
step 9000 (train): 2.118625641822815
Validating on a sample...
dev loss: 2.1248632144927977
Saving new best model!
step 9500 (train): 2.0432097799777984
Validating on a sample...
dev loss: 2.1365730714797975
Model is worse than the best so far. Current patience: 9
step 10000 (train): 2.0110645401477814
Validating on a sample...
dev loss: 2.0723536479473115
Saving new best model!
step 10500 (train): 1.9573512361049652
Validating on a sample...
dev loss: 2.0341351521015167
Saving new best model!
step 11000 (train): 1.914209439754486
Validating on a sample...
dev loss: 1.9747497606277467
Saving new best model!
step 11500 (train): 1.8466801697015762
Validating on a sample...
dev loss: 2.002696441411972
Model is worse than the best so far. Current patience: 9
step 12000 (train): 1.786162474632263
Validating on a sample...
dev loss: 1.9650258088111878
Saving new best model!
step 12500 (train): 1.7441310504674912
Validating on a sample...
dev loss: 1.8978291368484497
Saving new best model!
step 13000 (train): 1.703486807823181
Validating on a sample...
dev loss: 1.936644285917282
Model is worse than the best so far. Current patience: 9
step 13500 (train): 1.6808455278873444
Validating on a sample...
dev loss: 1.8705404233932494
